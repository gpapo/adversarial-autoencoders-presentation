% !TEX encoding = UTF-8
% !TEX spellcheck = en_US

\documentclass[10pt]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage[style=authoryear,backend=bibtex]{biblatex}
\addbibresource{bibliography.bib}

\setbeamertemplate{itemize item}{\color{black}$\circ$}
\setbeamertemplate{itemize subitem}{\color{black}$\bullet$}

% Math symbols
\DeclareMathOperator{\Cat}{Cat}
\DeclareMathOperator{\Norm}{\mathcal N}
\DeclareMathOperator{\Id}{\mathbf I}

\newcommand{\vect}[1]{\boldsymbol{#1}} % make bold vector names

\usepackage{booktabs}

\begin{document}
\author{Giovanni Papini}
\title{Adversarial Autoencoders (AAE)}
\subtitle{Adapted from \cite{makhzani2015adversarial}}
%\logo{}
\institute{Universit\'a degli Studi di Firenze}
\date{February 12, 2019}
\subject{autoencoders, neural network, AI}
%\setbeamercovered{transparent}
%\setbeamertemplate{navigation symbols}{}

\begin{frame}[plain]
\maketitle
\end{frame}

\begin{frame}{Abstract}
\begin{itemize}
  \item Probabilistic autoencoder
  \item Use of the GAN framework
  \item Removing ``holes'' in the latent space
  \item Many applications:
  \begin{itemize}
    \item semi-supervised learning
    \item disentanglement of style and content of images
    \item unsupervised clustering
    \item dimensionality reduction
    \item data visualization
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Context}
\begin{itemize}
  \item Scalable generative models to capture rich distributions
  \item Graphical models such as RBMs , DBNs, DBMs are based on MCMC algorithms for doing inference
  % via via sempre più imprecisi a causa dell'incapacità di mixing veloce tra le mode delle marginali
  \item VAEs, GANs, GMMNs are trained via direct back-propagation
  \item The AAE is trained with dual objectives:
  % AE classico non regolarizzato ottiene una codifica troppo arbitraria e poco compatta
  \begin{itemize}
    \item minimizing the reconstruction error (classic AE)
    \item adversarial criterion that matches the aggregated posterior distribution of the latent representation to an arbitrary prior
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{The AAE architecture}
\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{../images/aae-architecture-01.png}
\end{figure}
\[ p(\vect z) = \int_{\Omega_x} q(\vect z | \vect x) p_d(\vect x) \text d \vect x \]
\end{frame}

\begin{frame}{The encoder}
\begin{itemize}
  \setlength\itemsep{1.5em}
  \item \textbf{Deterministic} — (used in the paper)
  \item \textbf{Gaussian posterior} — $ z_i \sim \Norm(\mu_i(\vect x), \sigma_i(\vect x)) $
  \item \textbf{Universal approximator posterior} — $ \vect z | \vect x, \vect \eta \sim \delta(\vect z - f(\vect x, \vect \eta)) $ \\ where $ \vect \eta $ is random noise with a fixed distribution
  \[ q(\vect z | \vect x) = \int_{\Omega_\eta} q(\vect z | \vect x, \vect \eta) p_\eta(\vect \eta) \text d \vect \eta \]
\end{itemize}
\end{frame}

\begin{frame}{Relationship with VAEs}
\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{../images/aae-embedding-01.png}
\end{figure}
\end{frame}

\begin{frame}{Relationship with GANs and GMMNs}
\centering
\begin{tabular}{c c}
  \textbf{GAN} & \textbf{AAE} \\ \hline
  \parbox{0.45\linewidth}{\begin{itemize}
      \item imposes the data distribution to the output of a neural network
    \end{itemize}} &
  \parbox{0.45\linewidth}{\begin{itemize}
      \item relies on the autoencoder to capture the data distribution
      \item shapes a much lower dimensional space into a much simpler distribution
    \end{itemize}} \\
  \textbf{GMMN} & \textbf{AAE} \\ \hline
  \parbox{0.45\linewidth}{\begin{itemize}
      \item (first) trains a dropout autoencoder (then) fits a distribution in the code-space of the pretrained network
  \end{itemize}} &
  \parbox{0.45\linewidth}{\begin{itemize}
      \item uses adversarial training as a regularizer that shapes the code distribution while training the autoencoder from scratch
  \end{itemize}}
\end{tabular}
\end{frame}

\begin{frame}{Likelihood analysis}
\begin{itemize}
  \item Benchmarks on: MNIST, \href{../images/tfd.gif}{\underline{Toronto Face Dataset}} (TFD)
  \item Not direct likelihood measure, but lower bound approximation (KDE using Gaussian Parzen window, $ \sigma $ selected by cross-validation)
\end{itemize}
\begin{table}
  \centering
  \small
  %\includegraphics[width=\linewidth]{../images/performance-table-01.png}
  \begin{tabular}{l||c||c||c||c}
  	\toprule
  	             &   MNIST(10K)    & MNIST(10M) &    TFD(10K)     & TFD(10M) \\ \midrule
  	DBN          &  $ 138 \pm 2 $  &     -      & $ 1909 \pm 66 $ &    -     \\
  	Stacked CAE  & $ 121 \pm 1.6 $ &     -      & $ 2110 \pm 50 $ &    -     \\
  	Deep GSN     & $ 214 \pm 1.1 $ &     -      & $ 2890 \pm 29 $ &    -     \\
  	GAN          &  $ 225 \pm 2 $  &  $ 386 $   & $ 2057 \pm 26 $ &    -     \\
  	GMMN + AE    &  $ 282 \pm 2 $  &     -      & $ 2204 \pm 20 $ &    -     \\ \midrule
  	\textbf{AAE} &  $ 340 \pm 2 $  &  $ 427 $   & $ 2252 \pm 16 $ & $ 2522 $ \\ \bottomrule
  \end{tabular}
\end{table}
\end{frame}

\begin{frame}{Semi-supervised approach}{Architecture}
\begin{itemize}
  \item Incorporate one-hot vector in the latent representation
  \begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{../images/aae-architecture-02.png}
  \end{figure}
  \item The AAE disentangles \textit{style} features from \textit{content/semantic} features
  \item Experiments on MNIST and \href{../images/svhn.gif}{\underline{Street View House Number}} dataset (SVHN)
\end{itemize}
\end{frame}

\begin{frame}{Semi-supervised classification}{Architecture}
\begin{tabular}{cl}
  \begin{tabular}{c}
    \hspace{-0.5cm}\includegraphics[width=0.45\linewidth]{../images/aae-architecture-03.png}
  \end{tabular} &
  \begin{tabular}{l}
    \hspace{-1cm}\parbox{0.6\linewidth}{% change the width as appropriate
      \begin{itemize}
        \item Improve classification performance using both labeled and unlabeled data
        \item Assume the latent space is a mixed Categorical and Gaussian distribution
        \[ p(\vect y) = \Cat(\vect y) \quad p(\vect z) = \Norm(\vect z; 0, \Id) \]
        \item For each distribution a different adversarial network regularizes the latent representation
    \end{itemize}}
  \end{tabular}
\end{tabular}
\end{frame}

\begin{frame}{Semi-supervised classification}{Training}
3-phase training:
\begin{enumerate}
  \item \textit{reconstruction} — the AE updates the encoder $ q(\vect z, \vect y | \vect x) $ to minimize the reconstruction error
  \item \textit{regularization} — each adversarial network
  \begin{itemize}
    \item first updates the discriminative network to distinguish true samples from the generated samples
    \item then update the encoder to confuse the discriminative networks
  \end{itemize}
  \item \textit{semi-supervised classification} — the AE updates $ q(\vect z, \vect y | \vect x) $ to minimize the cross-entropy cost on a labeled minibatch
\end{enumerate}
\end{frame}

\begin{frame}{Semi-supervised classification}{Results}
\begin{table}
  \centering
  \scriptsize
  %\includegraphics[width=\linewidth]{../images/performance-table-02.png}
    \begin{tabular}{l||c|c|c||c}
    	\toprule
    	                &     MNIST(100)      &    MNIST(1000)    &    MNIST(All)     &     SVHN(1000)     \\ \midrule
    	NN Baseline     &      $ 25.80 $      &     $ 8.73 $      &     $ 1.25 $      &      $ 47.5 $      \\ \midrule
    	VAE (M1) + TSVM & $ 11.82 \pm 0.025 $ & $ 4.24 \pm 0.07 $ &         -         & $ 55.33 \pm 0.11 $ \\
    	VAE (M2)        & $ 11.97 \pm 1.71 $  & $ 3.60 \pm 0.56 $ &         -         & $ 36.02 \pm 0.10 $ \\
    	VAE (M1 + M2)   &  $ 3.33 \pm 0.14 $  & $ 2.40 \pm 0.02 $ &     $ 0.96 $      &     $ 24.63 $      \\
    	CatGAN          &  $ 1.91 \pm 0.1 $   & $ 1.73 \pm 0.18 $ &     $ 0.91 $      &         -          \\
    	Ladder Networks &  $ 1.06 \pm 0.37 $  & $ 0.84 \pm 0.08 $ & $ 0.57 \pm 0.02 $ &         -          \\
    	ADGM            &  $ 0.96 \pm 0.10 $  &         -         &         -         & $ 16.61 \pm 0.24 $ \\ \midrule
    	\textbf{AAE}    &  $ 1.90 \pm 0.10 $  & $ 1.60 \pm 0.08 $ & $ 0.85 \pm 0.02 $ & $ 17.70 \pm 0.30 $ \\ \bottomrule
    \end{tabular}
  \caption{Error rate on semi-supervised classification}
\end{table}
% AAE models are trained end-to-end while VAE models have to be trained one layer a time
\end{frame}

\begin{frame}{Unsupervised clustering}
\begin{itemize}
  \item Similar to the semi-supervised architecture, but without the semi-supervised training stage
  \item Not necessarily 10 classes, arbitrary number of clusters
  \item Benchmark criterion: find $ \arg\max_{\vect{x}_n} q(y_i | \vect x_n)  $ and assign label of $ x_n $ to all elements of $ i $-th cluster, then compute error rate based on the assigned class labels
\end{itemize}
\begin{table}
  \centering
  \small
  \begin{tabular}{l || c}
  	\toprule
  	                     & MNIST (Unsupervised) \\ \midrule
  	CatGAN (20 clusters) & $ 9.70 $             \\
  	AAE (16 clusters)    & $ 9.55 \pm 2.05 $    \\
  	AAE (30 clusters)    & $ 4.10 \pm 1.13 $    \\ \bottomrule
  \end{tabular}
  \caption{Error-rate on unsupervised clustering}
\end{table}
\end{frame}

\begin{frame}{Dimensionality reduction}{Architecture}
\begin{itemize}
  \item Typically, non-regularized autoencoders fracture the manifold resulting in very different codes for similar images
\end{itemize}
\begin{figure}
  \centering
  \includegraphics[width=0.6\linewidth]{../images/aae-architecture-04.png}
\end{figure}
\end{frame}

\begin{frame}{Conclusions}
\end{frame}

\begin{frame}{References}
\nocite{bengio2013better}
\nocite{bengio2014deep}
\nocite{goodfellow2014generative}
\nocite{kingma2013auto}
\printbibliography
\end{frame}

\end{document}